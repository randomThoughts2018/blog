---
title: "Learning Rate Finder and Snapshot Ensembles"
date: 2018-02-17T20:17:03Z
categories: ["deep-learning", "hyperparameters"]
draft: true
---

Understanding loss function topology using cyclical learning rates. Neural networks are universal approximation functions. Given enough training data and time, even a signle layer neural network can learn any mapping between inputs and outputs.

> * Papers to read - references: 
> * [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186)
> * [Exploring loss function topology with cyclical learning rates](https://arxiv.org/abs/1702.04283v1)
> * [Snapshot Ensembles: Train 1, get M for free](https://arxiv.org/abs/1704.00109v1)
> * [SGDR: Stochastic Gradient Descent with Warm Restarts](https://arxiv.org/abs/1608.03983v5)


